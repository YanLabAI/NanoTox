{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('nanoplastic_data.xlsx')\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9], \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'metric': ['euclidean', 'manhattan']  \n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "\n",
    "grid_search.fit(Xtrain, Ytrain)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cv: 0.31782682176148896 TEST: 0.483842333209731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,cross_val_predict,train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "X = pd.read_excel('nanoplastic_std_x0.xlsx')\n",
    "y = pd.read_excel('nanoplastic_std_y.xlsx')['Cell viability']\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=60)\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "CV_score = cross_val_score(knn, Xtrain, Ytrain, cv=5).mean()\n",
    "regressor = knn.fit(Xtrain, Ytrain)\n",
    "score_test = regressor.score(Xtest,Ytest)\n",
    "print(\"5cv:\",CV_score,\"TEST:\",score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cv: 0.3560757159271544 TEST: 0.35072861841864333\n",
      "5cv: 0.41372601755489447 TEST: 0.3672831948156128\n",
      "5cv: 0.3913212040604427 TEST: 0.2697743176016628\n",
      "5cv: 0.38991296968027944 TEST: 0.33064753332615626\n",
      "5cv: 0.3775159841047693 TEST: 0.41185643219689405\n",
      "5cv: 0.34397120384772367 TEST: 0.4567302912138327\n",
      "5cv: 0.3898543309015642 TEST: 0.301103065516484\n",
      "5cv: 0.3397536258871468 TEST: 0.4995598455539606\n",
      "5cv: 0.35556341613738096 TEST: 0.46318938000293564\n",
      "5cv: 0.3796068343718466 TEST: 0.39703437953812426\n",
      "5cv: 0.3167422498417346 TEST: 0.39790337646883933\n",
      "5cv: 0.37695414823774415 TEST: 0.43400510816287274\n",
      "5cv: 0.3860450757920155 TEST: 0.36560779596824855\n",
      "5cv: 0.39203819296305653 TEST: 0.3318240035486756\n",
      "5cv: 0.3894683803212602 TEST: 0.33894927447573353\n",
      "5cv: 0.3104028506854228 TEST: 0.5189413457575571\n",
      "5cv: 0.32846194247667193 TEST: 0.47107639241704746\n",
      "5cv: 0.3538969261303707 TEST: 0.3695966121051568\n",
      "5cv: 0.3889549621510307 TEST: 0.393554526388585\n",
      "5cv: 0.3561858888017295 TEST: 0.32952817347461694\n",
      "5cv: 0.37374926687066096 TEST: 0.355021411605267\n",
      "5cv: 0.35824537599721185 TEST: 0.4884882463138538\n",
      "5cv: 0.3628978656277277 TEST: 0.502672934190137\n",
      "5cv: 0.34377754656645737 TEST: 0.498772120893468\n",
      "5cv: 0.3810988509968075 TEST: 0.3985302468398296\n",
      "5cv: 0.3440975665003473 TEST: 0.42836015965588947\n",
      "5cv: 0.34725679747380916 TEST: 0.43131464774766737\n",
      "5cv: 0.3469956263581223 TEST: 0.4641353446381993\n",
      "5cv: 0.4151628950226451 TEST: 0.31737013548888293\n",
      "5cv: 0.3406875721077096 TEST: 0.43471311342547947\n",
      "5cv: 0.3754552963302256 TEST: 0.45532718193733723\n",
      "5cv: 0.3743677515517506 TEST: 0.4221088134209867\n",
      "5cv: 0.350989110746133 TEST: 0.5171261013246651\n",
      "5cv: 0.3805797657518831 TEST: 0.39691260332415323\n",
      "5cv: 0.4393565049549891 TEST: 0.18042317503825345\n",
      "5cv: 0.39313613953784615 TEST: 0.3039425881000186\n",
      "5cv: 0.37408110686087614 TEST: 0.4689850936275656\n",
      "5cv: 0.3761407281261131 TEST: 0.4525143141303928\n",
      "5cv: 0.3947300239021466 TEST: 0.35345026887895337\n",
      "5cv: 0.3802748692310132 TEST: 0.4510431232983033\n",
      "5cv: 0.31921737255284127 TEST: 0.4742442216852696\n",
      "5cv: 0.3507260528046108 TEST: 0.4149380951380359\n",
      "5cv: 0.3576672054329328 TEST: 0.4686296961776215\n",
      "5cv: 0.3725752883893957 TEST: 0.3812981805615864\n",
      "5cv: 0.3669898339888363 TEST: 0.4005601162334812\n",
      "5cv: 0.34929506135192395 TEST: 0.42433556101292946\n",
      "5cv: 0.3694163643479501 TEST: 0.3867965090694536\n",
      "5cv: 0.33028280678032496 TEST: 0.4748634414177717\n",
      "5cv: 0.37331392351394643 TEST: 0.32493281032078236\n",
      "5cv: 0.380676840292087 TEST: 0.41953926161848487\n",
      "5cv: 0.3545570677303583 TEST: 0.4045718273355209\n",
      "5cv: 0.40174130215819714 TEST: 0.27733842833890876\n",
      "5cv: 0.38443800788488824 TEST: 0.40024995525026585\n",
      "5cv: 0.38633543089067224 TEST: 0.49092582685315866\n",
      "5cv: 0.3847969646707904 TEST: 0.4193997737745774\n",
      "5cv: 0.3711979901545702 TEST: 0.44551837476426814\n",
      "5cv: 0.36502336880542186 TEST: 0.44288269050707263\n",
      "5cv: 0.350234141202252 TEST: 0.45757540763538607\n",
      "5cv: 0.3498523575770497 TEST: 0.4710163388234698\n",
      "5cv: 0.33267312108466174 TEST: 0.4316629744082531\n",
      "5cv: 0.31782682176148896 TEST: 0.483842333209731\n",
      "5cv: 0.39349017022501465 TEST: 0.3859750822121285\n",
      "5cv: 0.364643790860872 TEST: 0.37675407443746256\n",
      "5cv: 0.3517450014334839 TEST: 0.3882336679365591\n",
      "5cv: 0.3609886726671858 TEST: 0.42428962963717143\n",
      "5cv: 0.41410940039775096 TEST: 0.30130812980862476\n",
      "5cv: 0.3188435977742094 TEST: 0.4300739814812282\n",
      "5cv: 0.33438740619216045 TEST: 0.38565976481867514\n",
      "5cv: 0.395121524323579 TEST: 0.3414791397659612\n",
      "5cv: 0.36178434808543986 TEST: 0.33124600078423216\n",
      "5cv: 0.3410892851914659 TEST: 0.46268506077432336\n",
      "5cv: 0.3429088492584349 TEST: 0.4935405254551217\n",
      "5cv: 0.36255644831206424 TEST: 0.4229316310583502\n",
      "5cv: 0.3594546109539685 TEST: 0.4313456948470611\n",
      "5cv: 0.31692231368442875 TEST: 0.5556325321924\n",
      "5cv: 0.36423930125902315 TEST: 0.4477353506932332\n",
      "5cv: 0.33665100696415673 TEST: 0.3639702257187952\n",
      "5cv: 0.3402202462838141 TEST: 0.4607772649973747\n",
      "5cv: 0.35242908065920425 TEST: 0.5095884468300302\n",
      "5cv: 0.3485385581522874 TEST: 0.40859872334145053\n",
      "5cv: 0.34691094064026706 TEST: 0.5038911133553012\n",
      "5cv: 0.37196811817525977 TEST: 0.30435184073844623\n",
      "5cv: 0.33690122221094965 TEST: 0.4825846419565135\n",
      "5cv: 0.36341622058034684 TEST: 0.474459576627174\n",
      "5cv: 0.36712285963335656 TEST: 0.35901272101542137\n",
      "5cv: 0.3659489316925536 TEST: 0.31429027316348823\n",
      "5cv: 0.35300774883629055 TEST: 0.35658417027004874\n",
      "5cv: 0.3507216746082723 TEST: 0.48815600508059687\n",
      "5cv: 0.3712282911466766 TEST: 0.35081201584153987\n",
      "5cv: 0.36517626382915613 TEST: 0.416056599882487\n",
      "5cv: 0.3457249019757015 TEST: 0.47474104513445703\n",
      "5cv: 0.3105959102898031 TEST: 0.47370726117648077\n",
      "5cv: 0.3647642706747757 TEST: 0.3887881624013476\n",
      "5cv: 0.3729107397442305 TEST: 0.3860678845782032\n",
      "5cv: 0.3606444871525115 TEST: 0.49427013825392874\n",
      "5cv: 0.37017697109586256 TEST: 0.41129383888857884\n",
      "5cv: 0.37008685965162186 TEST: 0.4122243757051566\n",
      "5cv: 0.3791547642751249 TEST: 0.3706481864539509\n",
      "5cv: 0.3465439365900932 TEST: 0.3387497801323295\n",
      "5cv: 0.3760604648337593 TEST: 0.40961649387178223\n",
      "5cv: 0.38276051617120094 TEST: 0.4403362538246638\n",
      "5cv: 0.3475388780228298 TEST: 0.23322778536471478\n",
      "5cv: 0.36400232800994387 TEST: 0.4223375211235646\n",
      "5cv: 0.3557586456867444 TEST: 0.37535557453974744\n",
      "5cv: 0.37146890775622365 TEST: 0.42289048111665695\n",
      "5cv: 0.3780422324441811 TEST: 0.47115965998517884\n",
      "5cv: 0.3447393075838292 TEST: 0.40775464171866804\n",
      "5cv: 0.4070221140574063 TEST: 0.31063159627007564\n",
      "5cv: 0.34389380098454786 TEST: 0.37636415010277235\n",
      "5cv: 0.37518064704051407 TEST: 0.4309898522917154\n",
      "5cv: 0.382930002341372 TEST: 0.4136007049530457\n",
      "5cv: 0.35676914435450957 TEST: 0.4185956189654655\n",
      "5cv: 0.36413455056293415 TEST: 0.2997590870377338\n",
      "5cv: 0.36625348399032404 TEST: 0.45043808494961246\n",
      "5cv: 0.36323346378716226 TEST: 0.46154994031512875\n",
      "5cv: 0.40368201165557754 TEST: 0.3202047539569649\n",
      "5cv: 0.39073048406415545 TEST: 0.2805343840579586\n",
      "5cv: 0.3449161970271353 TEST: 0.4704281431593954\n",
      "5cv: 0.39536598789816113 TEST: 0.35886693797671554\n",
      "5cv: 0.36367261517766936 TEST: 0.3575588972368242\n",
      "5cv: 0.36781251825320327 TEST: 0.42418593252627856\n",
      "5cv: 0.370279897631411 TEST: 0.3520655538306424\n",
      "5cv: 0.347421493593732 TEST: 0.507536585881823\n",
      "5cv: 0.3541451848258198 TEST: 0.5135181359743541\n",
      "5cv: 0.3373786520163583 TEST: 0.47868376588510764\n",
      "5cv: 0.3818027480308512 TEST: 0.4258369594557664\n",
      "5cv: 0.3771261812137586 TEST: 0.3514252569670776\n",
      "5cv: 0.3941167908912885 TEST: 0.3006403569701943\n",
      "5cv: 0.3701445654988129 TEST: 0.48293277668833456\n",
      "5cv: 0.36277165689230867 TEST: 0.45083701428837786\n",
      "5cv: 0.3666710898388429 TEST: 0.4624209052320598\n",
      "5cv: 0.3271287021721856 TEST: 0.5140899171397055\n",
      "5cv: 0.39236872309546633 TEST: 0.32240047365307933\n",
      "5cv: 0.37569140353255 TEST: 0.3884149985877904\n",
      "5cv: 0.3580531530556154 TEST: 0.5004589779380657\n",
      "5cv: 0.3794734993127916 TEST: 0.3725134348884461\n",
      "5cv: 0.3828477146995534 TEST: 0.3674731379837216\n",
      "5cv: 0.41024784025600025 TEST: 0.21724839514794647\n",
      "5cv: 0.34642434151407975 TEST: 0.41387532331641963\n",
      "5cv: 0.3506391352125884 TEST: 0.4132840250101085\n",
      "5cv: 0.34460811548582104 TEST: 0.47130082370282445\n",
      "5cv: 0.3566531874915181 TEST: 0.43625619328519105\n",
      "5cv: 0.36972241731396716 TEST: 0.38224634760375775\n",
      "5cv: 0.3454010545121816 TEST: 0.5309322090758486\n",
      "5cv: 0.3683362073234074 TEST: 0.32709093221836316\n",
      "5cv: 0.3489205960649494 TEST: 0.3660896577196855\n",
      "5cv: 0.3409769363456308 TEST: 0.3168501194135481\n",
      "5cv: 0.3711631646679917 TEST: 0.3940983663937436\n",
      "5cv: 0.401215760627388 TEST: 0.3547925619769655\n",
      "5cv: 0.36236024796983096 TEST: 0.43531984553646186\n",
      "5cv: 0.339877917960343 TEST: 0.46623803827486565\n",
      "5cv: 0.3320478142253023 TEST: 0.5284515405196083\n",
      "5cv: 0.3596141866628405 TEST: 0.356587723121106\n",
      "5cv: 0.31810838191102764 TEST: 0.49626344986114923\n",
      "5cv: 0.31176491173170817 TEST: 0.5332947583679487\n",
      "5cv: 0.40796142762125676 TEST: 0.38045342247198866\n",
      "5cv: 0.3661418796829006 TEST: 0.3638627951368807\n",
      "5cv: 0.358029134716981 TEST: 0.25715586354626785\n",
      "5cv: 0.38131604924766976 TEST: 0.3357940079688234\n",
      "5cv: 0.3156801556892598 TEST: 0.4962641970158178\n",
      "5cv: 0.35379464043815684 TEST: 0.3336206834462804\n",
      "5cv: 0.3969856785032408 TEST: 0.3689581467489461\n",
      "5cv: 0.3948708616296538 TEST: 0.3178192894414176\n",
      "5cv: 0.35901429610136576 TEST: 0.46982431571833705\n",
      "5cv: 0.36611931485628035 TEST: 0.3174166295258477\n",
      "5cv: 0.3656309095300537 TEST: 0.46505611288982573\n",
      "5cv: 0.3446172423010151 TEST: 0.5029807185670323\n",
      "5cv: 0.3782448997634348 TEST: 0.4071300240303961\n",
      "5cv: 0.38017696042614213 TEST: 0.4056322377425752\n",
      "5cv: 0.3783234189136767 TEST: 0.44030241021964145\n",
      "5cv: 0.38329684706877076 TEST: 0.3806502221198089\n",
      "5cv: 0.39220304577161513 TEST: 0.3368574153392867\n",
      "5cv: 0.3904953697631911 TEST: 0.3832248172106655\n",
      "5cv: 0.39119068472007046 TEST: 0.41460612023696897\n",
      "5cv: 0.3763021564290783 TEST: 0.2926713735239672\n",
      "5cv: 0.36498521147793045 TEST: 0.5132139695060005\n",
      "5cv: 0.35699112439128994 TEST: 0.4072371044348033\n",
      "5cv: 0.36191972014706514 TEST: 0.3901123992939163\n",
      "5cv: 0.3298215080216344 TEST: 0.539693258165868\n",
      "5cv: 0.37843407390247735 TEST: 0.40191690926322476\n",
      "5cv: 0.37367713267240354 TEST: 0.4355266360757716\n",
      "5cv: 0.3640219996951622 TEST: 0.4124286264669358\n",
      "5cv: 0.41192007241216516 TEST: 0.35083992772929007\n",
      "5cv: 0.3730764320770169 TEST: 0.392345678286798\n",
      "5cv: 0.353392290977767 TEST: 0.5187816087084857\n",
      "5cv: 0.3372899432348825 TEST: 0.4514987767724512\n",
      "5cv: 0.38549667308972435 TEST: 0.37339637318554797\n",
      "5cv: 0.36047870135111715 TEST: 0.3860127043680126\n",
      "5cv: 0.3316620511548992 TEST: 0.4343911298542611\n",
      "5cv: 0.3790088554021017 TEST: 0.3842757124916253\n",
      "5cv: 0.3673003851138942 TEST: 0.38660137179376275\n",
      "5cv: 0.36423387373011024 TEST: 0.36518743036990087\n",
      "5cv: 0.38280271571715313 TEST: 0.3635226047269614\n",
      "5cv: 0.36011810031278774 TEST: 0.3405737600231693\n",
      "5cv: 0.3404760968148499 TEST: 0.4282869777701571\n",
      "5cv: 0.32080622290718974 TEST: 0.4110843156466255\n",
      "5cv: 0.37010294757634316 TEST: 0.3319522022327266\n",
      "5cv: 0.31782111589049794 TEST: 0.4759159149200123\n",
      "5cv: 0.3441826324006484 TEST: 0.5013414706170662\n",
      "5cv: 0.36089598739192763 TEST: 0.4561931902470474\n",
      "5cv: 0.3328683162934431 TEST: 0.4166444392790293\n",
      "5cv: 0.3463950331434207 TEST: 0.4134469731072409\n",
      "5cv: 0.3384719768058943 TEST: 0.48925022614859615\n",
      "5cv: 0.3896851022766693 TEST: 0.35684174762205234\n",
      "5cv: 0.3654545837015218 TEST: 0.4194534856050114\n",
      "5cv: 0.3771522893192426 TEST: 0.43467327651337706\n",
      "5cv: 0.32017591155692193 TEST: 0.40413930334688786\n",
      "5cv: 0.36923060524837126 TEST: 0.35346783825926975\n",
      "5cv: 0.37855854048895493 TEST: 0.30566615372326844\n",
      "5cv: 0.41699387280479494 TEST: 0.24656681464789554\n",
      "5cv: 0.37951847910878916 TEST: 0.4582816144618036\n",
      "5cv: 0.35191691652051654 TEST: 0.3998678437667077\n",
      "5cv: 0.3688394713352673 TEST: 0.36486260296445616\n",
      "5cv: 0.41222524905399816 TEST: 0.21072078453281085\n",
      "5cv: 0.3936688655721464 TEST: 0.34663915230056186\n",
      "5cv: 0.3727599771210156 TEST: 0.38670823803891274\n",
      "5cv: 0.3205327796004512 TEST: 0.4997403806802543\n",
      "5cv: 0.3546248594683738 TEST: 0.4177077142925788\n",
      "5cv: 0.34213981950153904 TEST: 0.45060777409063235\n",
      "5cv: 0.35729597878326474 TEST: 0.31878432355580266\n",
      "5cv: 0.38770149596300574 TEST: 0.37377148410882755\n",
      "5cv: 0.3371046357958796 TEST: 0.4733938071598355\n",
      "5cv: 0.35932351510411 TEST: 0.4268926473896688\n",
      "5cv: 0.36834218396302887 TEST: 0.45596320877430285\n",
      "5cv: 0.3877742033867585 TEST: 0.32697240862488786\n",
      "5cv: 0.35174289179082807 TEST: 0.413070765866389\n",
      "5cv: 0.35362134459399897 TEST: 0.30677869863737606\n",
      "5cv: 0.36103302914996027 TEST: 0.39295774482884616\n",
      "5cv: 0.3314992677978692 TEST: 0.48362312468228397\n",
      "5cv: 0.37730155081572586 TEST: 0.32605516669284884\n",
      "5cv: 0.34445982950851317 TEST: 0.3383708386327745\n",
      "5cv: 0.353718549272973 TEST: 0.4775999845298642\n",
      "5cv: 0.3815929209761223 TEST: 0.37854108517759477\n",
      "5cv: 0.41640297288391126 TEST: 0.32281613279555843\n",
      "5cv: 0.32006471855130714 TEST: 0.46944727050470536\n",
      "5cv: 0.34389710177118965 TEST: 0.33280566981810833\n",
      "5cv: 0.33720619507940935 TEST: 0.4310973576937267\n",
      "5cv: 0.39715227052803737 TEST: 0.4693183466764975\n",
      "5cv: 0.3665507969941009 TEST: 0.4244666170909267\n",
      "5cv: 0.3921833094637538 TEST: 0.38996036149747526\n",
      "5cv: 0.36730556263342357 TEST: 0.4026855233842792\n",
      "5cv: 0.3635813779001828 TEST: 0.4358560769019074\n",
      "5cv: 0.32506089896649887 TEST: 0.43710476745298643\n",
      "5cv: 0.3723300919415807 TEST: 0.3873473691901087\n",
      "5cv: 0.37555622654583803 TEST: 0.3934366575977153\n",
      "5cv: 0.3697973346200646 TEST: 0.4862692551176465\n",
      "5cv: 0.3677584002612179 TEST: 0.3661103546594885\n",
      "5cv: 0.3745052332675269 TEST: 0.4663139803795231\n",
      "5cv: 0.37641009569926875 TEST: 0.4421534343170178\n",
      "5cv: 0.40296535212587764 TEST: 0.28895226223029136\n",
      "5cv: 0.33112016386192195 TEST: 0.4143468921321465\n",
      "5cv: 0.34817642934698884 TEST: 0.4100280361449832\n",
      "5cv: 0.35466993317356243 TEST: 0.46418318578639417\n",
      "5cv: 0.3362242343306135 TEST: 0.48365378925911684\n",
      "5cv: 0.4020735560592333 TEST: 0.37716879112051327\n",
      "5cv: 0.3568756679737965 TEST: 0.4318313584870286\n",
      "5cv: 0.3763156447001911 TEST: 0.3570376300795757\n",
      "5cv: 0.3515691074662626 TEST: 0.37194414667825393\n",
      "5cv: 0.3617656405449298 TEST: 0.3852017115520533\n",
      "5cv: 0.357346112246731 TEST: 0.43322597278264374\n",
      "5cv: 0.3224645123026561 TEST: 0.46416014271346606\n",
      "5cv: 0.3617086667016526 TEST: 0.40627989952596655\n",
      "5cv: 0.35009280873799886 TEST: 0.39833009719085044\n",
      "5cv: 0.3903399562841572 TEST: 0.3563204303814924\n",
      "5cv: 0.4140227606689912 TEST: 0.2356749480331103\n",
      "5cv: 0.3707419503373458 TEST: 0.389595690259425\n",
      "5cv: 0.39091403003333414 TEST: 0.49521074604915205\n",
      "5cv: 0.36980433638258503 TEST: 0.5706346843002015\n",
      "5cv: 0.40251005593379574 TEST: 0.33087527741121714\n",
      "5cv: 0.36343976498426533 TEST: 0.45038051224611186\n",
      "5cv: 0.35204509483483576 TEST: 0.4847492016239757\n",
      "5cv: 0.33137289880891857 TEST: 0.5091292457066504\n",
      "5cv: 0.33334931392762135 TEST: 0.45169856583845247\n",
      "5cv: 0.3766098510151873 TEST: 0.46584976715737036\n",
      "5cv: 0.42247620629810784 TEST: 0.133379811878927\n",
      "5cv: 0.3854266162935763 TEST: 0.3711689299299811\n",
      "5cv: 0.3579884664921372 TEST: 0.3876943184771321\n",
      "5cv: 0.3961235243713991 TEST: 0.41847732390417325\n",
      "5cv: 0.34550636377553146 TEST: 0.47635518566906265\n",
      "5cv: 0.31868046612304857 TEST: 0.40037064243311116\n",
      "5cv: 0.3433762271099097 TEST: 0.3862339746093508\n",
      "5cv: 0.3816201440076278 TEST: 0.34909594970032154\n",
      "5cv: 0.34616520109790383 TEST: 0.49095928569690117\n",
      "5cv: 0.3858594493926096 TEST: 0.3623512746552424\n",
      "5cv: 0.4145200619669934 TEST: 0.36503348515250655\n",
      "5cv: 0.35021640901028717 TEST: 0.34763563777042117\n",
      "5cv: 0.3685577839527706 TEST: 0.43429840450867807\n",
      "5cv: 0.3810805730527105 TEST: 0.447176577061398\n",
      "5cv: 0.3395647144063673 TEST: 0.5210497953664492\n",
      "5cv: 0.3512561750695347 TEST: 0.3913242633207398\n",
      "5cv: 0.34129666822353155 TEST: 0.449353468077499\n",
      "5cv: 0.37317623274777023 TEST: 0.26310976444182865\n",
      "5cv: 0.3531894215883168 TEST: 0.47830051872453305\n",
      "5cv: 0.31705553416349036 TEST: 0.4702912696877173\n",
      "5cv: 0.348116382590266 TEST: 0.4332700018410739\n",
      "5cv: 0.3640459412538051 TEST: 0.3550358797164245\n",
      "5cv: 0.42583776868336753 TEST: 0.3299121068043256\n",
      "5cv: 0.33895850804113586 TEST: 0.4898892657948022\n",
      "5cv: 0.355100913393402 TEST: 0.3880223258398625\n",
      "5cv: 0.3601300623006264 TEST: 0.43168942478757366\n",
      "5cv: 0.3744318448515136 TEST: 0.35012606773741084\n",
      "5cv: 0.3317650599811698 TEST: 0.5001040389483962\n",
      "5cv: 0.3882314928137109 TEST: 0.4395112797388825\n",
      "5cv: 0.3598506088849227 TEST: 0.34879680696860527\n",
      "5cv: 0.36851028949526576 TEST: 0.49919677007914465\n",
      "5cv: 0.33368159880793735 TEST: 0.47662945586291694\n",
      "5cv: 0.3412542652398974 TEST: 0.49080026763597284\n",
      "5cv: 0.4042742612790061 TEST: 0.26836742538973957\n",
      "5cv: 0.336718515015991 TEST: 0.3977847698057594\n",
      "5cv: 0.3583633786280836 TEST: 0.37704060365475733\n",
      "5cv: 0.35173885538414595 TEST: 0.42141623344501833\n",
      "5cv: 0.3460414959411586 TEST: 0.4264415607505072\n",
      "5cv: 0.33304566030215293 TEST: 0.4353159507694996\n",
      "5cv: 0.3290206288407247 TEST: 0.5583011618595706\n",
      "5cv: 0.35752568894248843 TEST: 0.342044397444929\n",
      "5cv: 0.38480554565675673 TEST: 0.36407641726591844\n",
      "5cv: 0.3737160225776714 TEST: 0.3763355626605408\n",
      "5cv: 0.36763127891875913 TEST: 0.37691347840143785\n",
      "5cv: 0.3689490546666598 TEST: 0.42059661678087734\n",
      "5cv: 0.3716010494618504 TEST: 0.33698927967884285\n",
      "5cv: 0.40412208250441567 TEST: 0.37974346783485247\n",
      "5cv: 0.3709011385913711 TEST: 0.37485262786430973\n",
      "5cv: 0.3771984532634889 TEST: 0.45566721666084464\n",
      "5cv: 0.3670626883281119 TEST: 0.3781899992033191\n",
      "5cv: 0.3674546591386137 TEST: 0.3423171999552673\n",
      "5cv: 0.3479281482964603 TEST: 0.5026658170954261\n",
      "5cv: 0.38171798411454105 TEST: 0.3439016923597856\n",
      "5cv: 0.3707628271443934 TEST: 0.3876742888938265\n",
      "5cv: 0.345127946626277 TEST: 0.48226625897919606\n",
      "5cv: 0.3455162156045707 TEST: 0.428002769557875\n",
      "5cv: 0.37228867849615177 TEST: 0.33477965839113655\n",
      "5cv: 0.36384188564442105 TEST: 0.47749817045155074\n",
      "5cv: 0.4041842737025713 TEST: 0.28866755592616833\n",
      "5cv: 0.35089463801503307 TEST: 0.45463832841586105\n",
      "5cv: 0.3228385556358613 TEST: 0.4858792329844347\n",
      "5cv: 0.3793483737193357 TEST: 0.3478519087047579\n",
      "5cv: 0.37487501057055744 TEST: 0.4814181042092016\n",
      "5cv: 0.39506860309176434 TEST: 0.43885379360549404\n",
      "5cv: 0.3528746300199055 TEST: 0.4678633998301134\n",
      "5cv: 0.37236788935742576 TEST: 0.4199501345383876\n",
      "5cv: 0.36061330177910456 TEST: 0.2987285155852891\n",
      "5cv: 0.3593795735962012 TEST: 0.29673658044098483\n",
      "5cv: 0.41458401893251995 TEST: 0.24622542899294075\n",
      "5cv: 0.3263343643261833 TEST: 0.5507222703086438\n",
      "5cv: 0.35310710174452475 TEST: 0.47255283561826356\n",
      "5cv: 0.35374854053004484 TEST: 0.40058051301116826\n",
      "5cv: 0.3753569853809252 TEST: 0.44977790588850497\n",
      "5cv: 0.3514554726786018 TEST: 0.46501264773566575\n",
      "5cv: 0.34726944905020307 TEST: 0.4155391990935482\n",
      "5cv: 0.3226885347648115 TEST: 0.4788242964798388\n",
      "5cv: 0.34621076438741805 TEST: 0.46545142476429524\n",
      "5cv: 0.38797668811675395 TEST: 0.3439731802271031\n",
      "5cv: 0.40480453947026573 TEST: 0.3665157228816598\n",
      "5cv: 0.3324748985770173 TEST: 0.48844784512465844\n",
      "5cv: 0.38780471431362296 TEST: 0.4032916297268364\n",
      "5cv: 0.37490109075925254 TEST: 0.41584711180028455\n",
      "5cv: 0.3830463409257414 TEST: 0.4591727736885026\n",
      "5cv: 0.38921251541533214 TEST: 0.31780647532191464\n",
      "5cv: 0.3649291423980686 TEST: 0.41059175968957273\n",
      "5cv: 0.36504030620002315 TEST: 0.4245086124029922\n",
      "5cv: 0.41084893473239886 TEST: 0.35841425351162537\n",
      "5cv: 0.34339438638889214 TEST: 0.4439751927734238\n",
      "5cv: 0.36419643322832884 TEST: 0.45392682747228297\n",
      "5cv: 0.3514942360071373 TEST: 0.41933565189992883\n",
      "5cv: 0.3664347569256545 TEST: 0.36426337654651075\n",
      "5cv: 0.3664393894968371 TEST: 0.40793713284318356\n",
      "5cv: 0.37085784806717415 TEST: 0.4097016435445241\n",
      "5cv: 0.3687036120222047 TEST: 0.38475239357793445\n",
      "5cv: 0.38908439303828557 TEST: 0.433328404873763\n",
      "5cv: 0.3758290982495021 TEST: 0.35705564004633894\n",
      "5cv: 0.39404581690838114 TEST: 0.3067828510096605\n",
      "5cv: 0.38669981896137245 TEST: 0.35078708981747997\n",
      "5cv: 0.38244256647738556 TEST: 0.36048372150119623\n",
      "5cv: 0.34736951260535076 TEST: 0.47414561040086367\n",
      "5cv: 0.3523869171392997 TEST: 0.42839756945917173\n",
      "5cv: 0.34891826132452336 TEST: 0.40875049657757034\n",
      "5cv: 0.3641958849557866 TEST: 0.44082871097574505\n",
      "5cv: 0.4025253082769346 TEST: 0.3342901522463232\n",
      "5cv: 0.39262223495251725 TEST: 0.3337797918969294\n",
      "5cv: 0.3677058633334343 TEST: 0.3998725097119403\n",
      "5cv: 0.36065008186622566 TEST: 0.47964541160232244\n",
      "5cv: 0.34385869712174333 TEST: 0.40949416676195216\n",
      "5cv: 0.35366737377462865 TEST: 0.41984264914047\n",
      "5cv: 0.343067191674921 TEST: 0.3888220585073855\n",
      "5cv: 0.3840867235809965 TEST: 0.40462530822375276\n",
      "5cv: 0.35225902911688944 TEST: 0.47443350456452826\n",
      "5cv: 0.369103695450241 TEST: 0.46350018437957885\n",
      "5cv: 0.37395175043553897 TEST: 0.3837180728065632\n",
      "5cv: 0.40629104290403434 TEST: 0.30094477591944435\n",
      "5cv: 0.36745127880474376 TEST: 0.32526521817561804\n",
      "5cv: 0.37061789992272215 TEST: 0.4206231423949147\n",
      "5cv: 0.3662181813438937 TEST: 0.43332301304626164\n",
      "5cv: 0.3193804173619331 TEST: 0.43710167613060846\n",
      "5cv: 0.31607473106612893 TEST: 0.46797618912542993\n",
      "5cv: 0.40005770215945696 TEST: 0.37180531635006575\n",
      "5cv: 0.36898615325281225 TEST: 0.3825314582247742\n",
      "5cv: 0.36197195270868027 TEST: 0.4007534760976116\n",
      "5cv: 0.35642902884142885 TEST: 0.37283703856138617\n",
      "5cv: 0.3515535011017178 TEST: 0.46904194500552887\n",
      "5cv: 0.402660847004945 TEST: 0.3395039658113598\n",
      "5cv: 0.3466070881193468 TEST: 0.33221491350389\n",
      "5cv: 0.360161329351169 TEST: 0.33819707872941285\n",
      "5cv: 0.36915550798938196 TEST: 0.40275305820382634\n",
      "5cv: 0.3600111593918847 TEST: 0.4739712815062562\n",
      "5cv: 0.35934372386826574 TEST: 0.4348714442967231\n",
      "5cv: 0.366386276608629 TEST: 0.3918157995429443\n",
      "5cv: 0.356178255302479 TEST: 0.3718521113737667\n",
      "5cv: 0.383553505407182 TEST: 0.32137980175417247\n",
      "5cv: 0.35481271725464236 TEST: 0.4114124429411039\n",
      "5cv: 0.37635074848348066 TEST: 0.4259950674279189\n",
      "5cv: 0.3600581787944107 TEST: 0.3348425483019626\n",
      "5cv: 0.36774532484788036 TEST: 0.4244840069413106\n",
      "5cv: 0.39680397268362366 TEST: 0.3086232788966613\n",
      "5cv: 0.3501720761861845 TEST: 0.37723556008157\n",
      "5cv: 0.3773358336122835 TEST: 0.370651774467869\n",
      "5cv: 0.3758852346600487 TEST: 0.3610629177384691\n",
      "5cv: 0.3627488871516785 TEST: 0.3701794219018477\n",
      "5cv: 0.368194562676649 TEST: 0.3284961400204135\n",
      "5cv: 0.3595266139590712 TEST: 0.4734125479453304\n",
      "5cv: 0.35907399557363673 TEST: 0.3954103557622265\n",
      "5cv: 0.3231179288356236 TEST: 0.48036182455559995\n",
      "5cv: 0.38212990610302805 TEST: 0.39235058804165335\n",
      "5cv: 0.36297778315533 TEST: 0.4100068785500455\n",
      "5cv: 0.3724111636311583 TEST: 0.3608384166646652\n",
      "5cv: 0.3639481572152917 TEST: 0.3290005670673444\n",
      "5cv: 0.3427398831541574 TEST: 0.3805714194310651\n",
      "5cv: 0.3734622474784005 TEST: 0.37304033632752465\n",
      "5cv: 0.3945114021111385 TEST: 0.4282293049020136\n",
      "5cv: 0.3207884196967048 TEST: 0.4532603916727328\n",
      "5cv: 0.387356323441599 TEST: 0.4289549982435784\n",
      "5cv: 0.3434346280009979 TEST: 0.4824213396633815\n",
      "5cv: 0.41164663849375377 TEST: 0.22406119608760833\n",
      "5cv: 0.38361774738085197 TEST: 0.43556011977460074\n",
      "5cv: 0.34633039579271074 TEST: 0.4263989008657111\n",
      "5cv: 0.3805328447704178 TEST: 0.43416440121274735\n",
      "5cv: 0.37447929853698925 TEST: 0.38292243742874177\n",
      "5cv: 0.3585111930516363 TEST: 0.4141171359392538\n",
      "5cv: 0.33987170532846717 TEST: 0.48984390526024335\n",
      "5cv: 0.3465895961335307 TEST: 0.3580152285352476\n",
      "5cv: 0.354482939921101 TEST: 0.3999591259437849\n",
      "5cv: 0.34879831030138087 TEST: 0.402779747137193\n",
      "5cv: 0.3818295808123546 TEST: 0.4335153962760774\n",
      "5cv: 0.34230735835147474 TEST: 0.4514487852997001\n",
      "5cv: 0.3777167652508605 TEST: 0.3671030652055396\n",
      "5cv: 0.3264753885239426 TEST: 0.43301102144796444\n",
      "5cv: 0.3971803890014464 TEST: 0.45180560049544416\n",
      "5cv: 0.324440617486391 TEST: 0.4401558637095342\n",
      "5cv: 0.34816920501550824 TEST: 0.49795670033429484\n",
      "5cv: 0.3728791338020938 TEST: 0.45579778104474333\n",
      "5cv: 0.35956561295400336 TEST: 0.33419830740123657\n",
      "5cv: 0.3795311843447514 TEST: 0.3722651286953764\n",
      "5cv: 0.36357191081484824 TEST: 0.35582879317969507\n",
      "5cv: 0.35534378922660065 TEST: 0.3709658122459283\n",
      "5cv: 0.356302256327993 TEST: 0.4529353761342919\n",
      "5cv: 0.36696482913632356 TEST: 0.34547363688662225\n",
      "5cv: 0.33010579466285395 TEST: 0.5105101170632302\n",
      "5cv: 0.3779947341304288 TEST: 0.4038788767545325\n",
      "5cv: 0.368840446440576 TEST: 0.47094731455265015\n",
      "5cv: 0.3820446059964347 TEST: 0.4244251288500991\n",
      "5cv: 0.3122097134176965 TEST: 0.4571750266509158\n",
      "5cv: 0.35887179153504495 TEST: 0.4594033156266467\n",
      "5cv: 0.3412572502306371 TEST: 0.48613910970240115\n",
      "5cv: 0.3750314709494402 TEST: 0.2826207862455583\n",
      "5cv: 0.34922820237367325 TEST: 0.4548898199187942\n",
      "5cv: 0.3370295346801699 TEST: 0.49255189357027707\n",
      "5cv: 0.370291021278785 TEST: 0.3662209714357689\n",
      "5cv: 0.306943261589428 TEST: 0.4972867441805995\n",
      "5cv: 0.3345411369883127 TEST: 0.472425127524406\n",
      "5cv: 0.34925017570762146 TEST: 0.41449288660990424\n",
      "5cv: 0.35512289837739186 TEST: 0.44237668401894226\n",
      "5cv: 0.33609422280907025 TEST: 0.35258558740735957\n",
      "5cv: 0.36970773151083536 TEST: 0.4635376043146774\n",
      "5cv: 0.38184013583191706 TEST: 0.5400968867634637\n",
      "5cv: 0.3888477818486541 TEST: 0.3965504863187619\n",
      "5cv: 0.38140775630487667 TEST: 0.3864435717261555\n",
      "5cv: 0.4012954132449501 TEST: 0.3148033005136299\n",
      "5cv: 0.35323712574591015 TEST: 0.3755905126518304\n",
      "5cv: 0.35367327690530576 TEST: 0.43071927512765074\n",
      "5cv: 0.35968440356249387 TEST: 0.46949560443400384\n",
      "5cv: 0.3424791023304718 TEST: 0.5198547591542149\n",
      "5cv: 0.3321584095759679 TEST: 0.4266076442678378\n",
      "5cv: 0.37682717933478294 TEST: 0.4304756722713601\n",
      "5cv: 0.36128096191022535 TEST: 0.40255651666312275\n",
      "5cv: 0.33833342251476156 TEST: 0.3828633146451529\n",
      "5cv: 0.37986172976664284 TEST: 0.41319695791629607\n",
      "5cv: 0.35949237055758887 TEST: 0.3900599989189336\n",
      "5cv: 0.3466752608275806 TEST: 0.451464826146177\n",
      "5cv: 0.36838103191304794 TEST: 0.4266835565639481\n",
      "5cv: 0.3323467284495155 TEST: 0.43270127522339163\n",
      "5cv: 0.3231895179459891 TEST: 0.5171238742995551\n",
      "5cv: 0.3768669659582435 TEST: 0.3473783529131299\n",
      "5cv: 0.41815138948486713 TEST: 0.277577631938697\n",
      "5cv: 0.4072515231886754 TEST: 0.3354402579543029\n",
      "5cv: 0.3190974438937461 TEST: 0.4923533957051639\n",
      "5cv: 0.36724232730301726 TEST: 0.3951238192658424\n",
      "5cv: 0.3531523413956884 TEST: 0.31053662363237766\n",
      "5cv: 0.3811487884610463 TEST: 0.35006219684872164\n",
      "5cv: 0.35383832204121396 TEST: 0.5020139569366875\n",
      "5cv: 0.3535557472182753 TEST: 0.4212325081263255\n",
      "5cv: 0.3550197246709398 TEST: 0.46583983594696876\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=i)\n",
    "    rfc = KNeighborsRegressor()\n",
    "    CV_score = cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "    regressor = rfc.fit(Xtrain, Ytrain)\n",
    "    score_test = regressor.score(Xtest,Ytest)\n",
    "#     if CV < CV_score and test < score_test:\n",
    "#         CV = CV_score\n",
    "#         test = score_test\n",
    "    print(\"5cv:\",CV_score,\"TEST:\",score_test)\n",
    "    if CV_score>0.40 and score_test>0.4:\n",
    "        print(\"5cv:\",CV_score,\"TEST:\",score_test,\"random_state:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cv: 0.42583776868336753 TEST: 0.3299121068043256\n",
      "rmse_5CV 18.15711196752332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=296)\n",
    "\n",
    "rfc = KNeighborsRegressor(n_neighbors=5)\n",
    "CV_score = cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "regressor = rfc.fit(Xtrain, Ytrain)\n",
    "CV_predictions = cross_val_predict(rfc, Xtrain,Ytrain,cv=5)\n",
    "rmse = np.sqrt(mean_squared_error(Ytrain,CV_predictions))\n",
    "\n",
    "\n",
    "score_test = regressor.score(Xtest,Ytest)\n",
    "print(\"5cv:\",CV_score,\"TEST:\",score_test)\n",
    "print(\"rmse_5CV\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_5cv score:0.45279878033598564 n_neighbors_5cv:2\n",
      "Best_5cv score:0.45279878033598564 weights_5cv:uniform\n",
      "Best_5cv score:0.45456236746223444 algorithm_5cv:ball_tree\n",
      "Best_5cv score:0.45456236746223444 leaf_size_5cv:11\n",
      "r2_5cv: 0.39944634538537904 rmse_5CV 18.339917415434297 MAE_5CV 10.878557198521046\n",
      "test: 0.614859165970695\n",
      "rmse_test 14.430788918020689\n",
      "mae_test 8.978184147727273\n"
     ]
    }
   ],
   "source": [
    "score_5cv_all = []\n",
    "for i in range(0, 12, 1):\n",
    "    rfc =KNeighborsRegressor(n_neighbors=i+1)\n",
    "    score_5cv =cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "    score_5cv_all.append(score_5cv)\n",
    "    pass\n",
    "\n",
    "\n",
    "score_max_5cv = max(score_5cv_all)\n",
    "n_neighbors_5cv = range(0, 12,1)[score_5cv_all.index(max(score_5cv_all))]+1\n",
    "\n",
    "print(\"Best_5cv score:{}\".format(score_max_5cv),\n",
    "      \"n_neighbors_5cv:{}\".format(n_neighbors_5cv))\n",
    "\n",
    "\n",
    "score_5cv_all = []\n",
    "for i in ['uniform', 'distance']:\n",
    "    rfc = KNeighborsRegressor(weights=i\n",
    "                                , n_neighbors=n_neighbors_5cv)\n",
    "    score_5cv = cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "    score_5cv_all.append(score_5cv)\n",
    "    pass\n",
    "\n",
    "score_max_5cv = max(score_5cv_all)\n",
    "weights_5cv = ['uniform', 'distance'][score_5cv_all.index(score_max_5cv)]\n",
    "\n",
    "print(\"Best_5cv score:{}\".format(score_max_5cv),\n",
    "      \"weights_5cv:{}\".format(weights_5cv))\n",
    "\n",
    "score_5cv_all = []\n",
    "for i in ['brute', 'kd_tree','auto', 'ball_tree']:\n",
    "    rfc = KNeighborsRegressor(algorithm=i\n",
    "                                , weights=weights_5cv\n",
    "                                , n_neighbors=n_neighbors_5cv)\n",
    "    score_5cv = cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "    score_5cv_all.append(score_5cv)\n",
    "    pass\n",
    "\n",
    "score_max_5cv = max(score_5cv_all)\n",
    "algorithm_5cv = ['brute', 'kd_tree','auto', 'ball_tree'][score_5cv_all.index(score_max_5cv)]\n",
    "\n",
    "print(\"Best_5cv score:{}\".format(score_max_5cv),\n",
    "      \"algorithm_5cv:{}\".format(algorithm_5cv))\n",
    "\n",
    "score_5cv_all = []\n",
    "for i in range(0, 1000, 1):\n",
    "    rfc = KNeighborsRegressor(leaf_size=i+1\n",
    "                                , algorithm=algorithm_5cv\n",
    "                                , weights=weights_5cv\n",
    "                                , n_neighbors=n_neighbors_5cv)\n",
    "    score_5cv = cross_val_score(rfc, Xtrain, Ytrain, cv=5).mean()\n",
    "    score_5cv_all.append(score_5cv)\n",
    "    pass\n",
    "\n",
    "score_max_5cv = max(score_5cv_all)\n",
    "leaf_size_5cv = range(10, 1000, 1)[score_5cv_all.index(score_max_5cv)]+1\n",
    "\n",
    "print(\"Best_5cv score:{}\".format(score_max_5cv),\n",
    "      \"leaf_size_5cv:{}\".format(leaf_size_5cv))\n",
    "\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=82)\n",
    "\n",
    "knn = KNeighborsRegressor(leaf_size=leaf_size_5cv\n",
    "                                , algorithm=algorithm_5cv\n",
    "                                , weights=weights_5cv\n",
    "                                , n_neighbors=n_neighbors_5cv)\n",
    "\n",
    "CV_score = cross_val_score(knn, Xtrain, Ytrain, cv=5).mean()\n",
    "CV_predictions = cross_val_predict(knn, Xtrain,Ytrain,cv=5)\n",
    "rmse = np.sqrt(mean_squared_error(Ytrain,CV_predictions))\n",
    "mae = mean_absolute_error(Ytrain,CV_predictions)\n",
    "print(\"r2_5cv:\",CV_score,\"rmse_5CV\",rmse,\"MAE_5CV\",mae)\n",
    "expvspred_5cv = {'Exp': Ytrain, 'Pred':CV_predictions}\n",
    "pd.DataFrame(expvspred_5cv).to_excel('KNN_5fcv_pred.xlsx')\n",
    "\n",
    "'''\n",
    "Test set validation\n",
    "\n",
    "'''\n",
    "\n",
    "knn = KNeighborsRegressor(leaf_size=leaf_size_5cv\n",
    "                                , algorithm=algorithm_5cv\n",
    "                                , weights=weights_5cv\n",
    "                                , n_neighbors=n_neighbors_5cv)\n",
    "regressor = knn.fit(Xtrain, Ytrain)\n",
    "test_predictions = regressor.predict(Xtest)\n",
    "score_test = regressor.score(Xtest,Ytest)\n",
    "test_rmse = np.sqrt(mean_squared_error(Ytest,test_predictions))\n",
    "test_mae = mean_absolute_error(Ytest,test_predictions)\n",
    "print(\"test:\",score_test)\n",
    "print(\"rmse_test\",test_rmse)\n",
    "print(\"mae_test\",test_mae)\n",
    "expvspred_test = {'Exp':Ytest,'Pred':test_predictions}\n",
    "pd.DataFrame(expvspred_test).to_excel('KNN_test_pred.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47ba1233a695b5f93a12b4e1f6ace96ada2d207d279b8a4ebd01e46eaae8e613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
